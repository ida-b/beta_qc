{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 1: import functions #####\n",
    "from keras.layers import Dense, Flatten, BatchNormalization, Activation, Conv2D, Conv1D, AveragePooling2D, AveragePooling1D, Input, MaxPooling1D, Dropout\n",
    "from keras.models import load_model, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "from keras import backend as K, initializers, regularizers\n",
    "from keras.metrics import mean_absolute_error, mean_squared_error\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from numpy.random import seed; seed(473)\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed; set_random_seed(763)\n",
    "from keras.losses import binary_crossentropy\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "# from sklearn.externals.six import StringIO  \n",
    "from sklearn.tree import export_graphviz, DecisionTreeRegressor\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.utils import class_weight\n",
    "from scipy.stats import ttest_ind\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 2: load data #####\n",
    "x = np.load('data/d13_unfiltered_all_log_stdrd.npy')\n",
    "metaData = pd.read_csv('data/d13_annotations.csv', delimiter=';')\n",
    "y = metaData['D13 PDX/NKX'].values\n",
    "\n",
    "# filter out samples with NaN values\n",
    "nan_idx = np.isnan(y)\n",
    "x = x[~nan_idx]\n",
    "y = y[~nan_idx]\n",
    "samples = metaData['sample'].values[~nan_idx]\n",
    "\n",
    "print('Total number of samples: ', x.shape[0])\n",
    "\n",
    "# test_folds = np.load('data/d13_test_folds.npy', allow_pickle=True)\n",
    "# valid_folds = np.load('data/d13_valid_folds.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find amount of entries in y below 30.0, between 30.0 and 40.0 and above 40.0\n",
    "below_30 = np.sum(y < 30.0)\n",
    "between_30_40 = np.sum((y >= 30.0) & (y < 40.0))\n",
    "above_40 = np.sum(y >= 40.0)\n",
    "print('Below 30.0: ', below_30)\n",
    "print('Between 30.0 and 40.0: ', between_30_40)\n",
    "print('Above 40.0: ', above_40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folds = np.load('data/d13_test_folds.npy', allow_pickle=True)\n",
    "valid_folds = np.load('data/d13_valid_folds.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 3: define test and train sets #####\n",
    "# shuffle data\n",
    "idx = np.arange(x.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "shuffled_samples = samples[idx]\n",
    "test_folds = np.array_split(shuffled_samples, 7)\n",
    "\n",
    "# create 5-fold train-validation splits for each test fold\n",
    "valid_folds = []\n",
    "for test_fold in test_folds:\n",
    "    idx_train = np.isin(shuffled_samples, test_fold, invert=True)\n",
    "    samples_train = shuffled_samples[idx_train]\n",
    "    \n",
    "    valid_fold = np.array_split(samples_train, 5)\n",
    "    valid_folds.append(valid_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 4: define model #####\n",
    "\n",
    "def create_model(x, l2=0.001, dropout=1/3):\n",
    "    # input\n",
    "    model_input = Input(shape=x[0].shape)\n",
    "\n",
    "    # first convolution layer\n",
    "    model_output = Conv1D(3, \n",
    "                        kernel_size=1, \n",
    "                        kernel_initializer=initializers.RandomUniform(),\n",
    "                        kernel_regularizer=regularizers.l2(l2),\n",
    "                        activation=None)(model_input)\n",
    "    model_output = BatchNormalization()(model_output)\n",
    "    model_output = Activation(\"relu\")(model_output)\n",
    "\n",
    "    # # second convolution layer\n",
    "    # model_output = Conv1D(4,\n",
    "    #                     kernel_size=1,\n",
    "    #                     kernel_initializer=initializers.RandomUniform(),\n",
    "    #                     kernel_regularizer=regularizers.l2(l2),\n",
    "    #                     activation=None)(model_output)\n",
    "    # model_output = BatchNormalization()(model_output)\n",
    "    # model_output = Activation(\"relu\")(model_output)\n",
    "\n",
    "    # pooling layer\n",
    "    model_output = AveragePooling1D(pool_size=x.shape[1])(model_output)\n",
    "    model_output = Flatten()(model_output)\n",
    "\n",
    "    # # dropout\n",
    "    # model_output = Dropout(rate=1/4)(model_output)\n",
    "\n",
    "    # Dense layer\n",
    "    model_output = Dense(3, \n",
    "                        kernel_initializer=initializers.RandomUniform(),\n",
    "                        kernel_regularizer=regularizers.l2(l2),\n",
    "                        activation=None)(model_output)\n",
    "    model_output = BatchNormalization()(model_output)\n",
    "    model_output = Activation(\"relu\")(model_output)\n",
    "\n",
    "    # output layer\n",
    "    model_output = Dense(1, \n",
    "                        kernel_initializer=initializers.RandomUniform(),\n",
    "                        activation=None)(model_output)\n",
    "    model_output = BatchNormalization()(model_output)\n",
    "    model_output = Activation(\"linear\")(model_output)\n",
    "\n",
    "    return Model(inputs=model_input, outputs=model_output)\n",
    "\n",
    "# define function for plotting train and validation loss and accuracy\n",
    "def plot_loss(history):\n",
    "    # increase font size\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('train vs validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def calc_mean_std(history):\n",
    "    mean = np.array(history).mean(axis=0)\n",
    "    std = np.array(history).std(axis=0)\n",
    "    return mean, std\n",
    "\n",
    "def plot_error_bar(train_mean, train_std, val_mean, val_std, ylabel, ax, leg_loc='best'):\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.errorbar(range(1, len(train_mean) + 1), train_mean, yerr=train_std, label='train')\n",
    "    ax.errorbar(range(1, len(val_mean) + 1), val_mean, yerr=val_std, label='validation')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    # ax.set_xticks(range(1, len(train_mean) + 1))\n",
    "    ax.set_ylim(bottom=0)\n",
    "    ax.legend(loc=leg_loc)\n",
    "\n",
    "def custom_mse(y_true, y_pred):\n",
    "    return np.mean(np.square(y_pred - y_true), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 5: train and test model #####\n",
    "\n",
    "# implement a learning rate schedule which starts at 0.1, then decreases with a rate of 0.9 in 100 steps\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 100:\n",
    "        return lr*0.97\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "for test_round, test_fold in enumerate(test_folds):\n",
    "    print(f\"Test fold {test_round+1}/{len(test_folds)}\")\n",
    "    \n",
    "    idx_test = np.isin(samples, test_fold)\n",
    "    idx_train = np.invert(idx_test)\n",
    "\n",
    "    x_train = x[idx_train,:,:]; y_train = y[idx_train]; samples_train = samples[idx_train]\n",
    "    x_test = x[idx_test,:,:]; y_test = y[idx_test]; samples_test = samples[idx_test]\n",
    "\n",
    "    # Lists to store evaluation results\n",
    "    val_loss_history = []; val_mae_history = []\n",
    "    train_loss_history = []; train_mae_history = []\n",
    "\n",
    "    mae_best_epoch = []; loss_best_epoch = []\n",
    "\n",
    "    # for fold, (train_indices, valid_indices) in enumerate(kf.split(x_train)):\n",
    "    for fold, valid_samples in enumerate(valid_folds[test_round]):\n",
    "        print(f\"Training fold {fold+1}/{len(valid_folds[test_round])}\")\n",
    "\n",
    "        valid_indices = np.isin(samples_train, valid_samples)\n",
    "        train_indices = np.invert(valid_indices)\n",
    "        \n",
    "        x_train_fold = x_train[train_indices]\n",
    "        y_train_fold = y_train[train_indices]\n",
    "        x_valid_fold = x_train[valid_indices]\n",
    "        y_valid_fold = y_train[valid_indices]\n",
    "        \n",
    "        # Build the model (same as your previous code)\n",
    "        model = create_model(x_train, l2=0.00)\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.1))\n",
    "\n",
    "        lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "        checkpointer = ModelCheckpoint(filepath=f'saved_weights_d13_pred_testfold{test_round+1}_fold{fold+1}.hdf5', \n",
    "                                    monitor='val_loss', verbose=0, \n",
    "                                    save_best_only=True)\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(x_train_fold, y_train_fold,\n",
    "                            batch_size=5,\n",
    "                            epochs=100, \n",
    "                            verbose=0,\n",
    "                            callbacks=[checkpointer, lr_scheduler],\n",
    "                            validation_data=([x_valid_fold], y_valid_fold)\n",
    "                            )\n",
    "        \n",
    "        plot_loss(model.history)\n",
    "        \n",
    "        # Store validation metrics for each fold\n",
    "        val_loss_history.append(history.history['val_loss'])\n",
    "        train_loss_history.append(history.history['loss'])\n",
    "\n",
    "        # Find the epoch with the lowest validation loss\n",
    "        best_epoch = np.argmin(history.history['val_loss'])\n",
    "        loss_best_epoch.append(history.history['val_loss'][best_epoch])\n",
    "\n",
    "        # print validation accuracy and loss at best epoch\n",
    "        print(f\"Validation loss at best epoch: {history.history['val_loss'][best_epoch]:.2f}\")\n",
    "    \n",
    "    # print model with lowest validation loss and highest validation accuracy\n",
    "    print(f\"Model with lowest validation loss at best epoch: {np.argmin(loss_best_epoch)+1}\")\n",
    "\n",
    "    # Calculate average validation loss and accuracy for each model\n",
    "    avg_val_loss = np.array(val_loss_history).mean(axis=1)\n",
    "\n",
    "    # Choose the model with the highest average validation accuracy\n",
    "    print(f\"Model with lowest average validation loss: {np.argmin(avg_val_loss)+1}\")\n",
    "\n",
    "    # plot average validation loss\n",
    "    [train_mean, train_std] = calc_mean_std(train_loss_history)\n",
    "    [val_mean, val_std] = calc_mean_std(val_loss_history)\n",
    "    print(f\"Average train loss: {train_mean[-1]:.2f} +/- {train_std[-1]:.2f}\")\n",
    "    print(f\"Average validation loss: {val_mean[-1]:.2f} +/- {val_std[-1]:.2f}\")\n",
    "\n",
    "    fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "    plot_error_bar(train_mean, train_std, val_mean, val_std, 'loss', ax, leg_loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    # test models on test fold\n",
    "    y_5fold_pred = np.zeros((y_test.shape[0], 5))\n",
    "\n",
    "    for i in range(5):\n",
    "        final_model = load_model(f'saved_weights_d13_pred_testfold{test_round+1}_fold{i+1}.hdf5', compile=False)\n",
    "\n",
    "        # define loss function and optimizer\n",
    "        final_model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.1))\n",
    "\n",
    "        # generate ROC and AUC\n",
    "        y_scores = final_model.predict([x_test])\n",
    "        y_5fold_pred[:,i] = y_scores.flatten()\n",
    "\n",
    "    # print prediction and true label\n",
    "    df = pd.DataFrame({'sample': samples[idx_test],\n",
    "                        'model 1': y_5fold_pred[:,0],\n",
    "                        'model 2': y_5fold_pred[:,1],\n",
    "                        'model 3': y_5fold_pred[:,2],\n",
    "                        'model 4': y_5fold_pred[:,3],\n",
    "                        'model 5': y_5fold_pred[:,4],\n",
    "                        'true': y_test})\n",
    "    print(df)\n",
    "    \n",
    "    # compute mean absolute error and mean squared error\n",
    "    mae = np.abs(df.iloc[:,1:-1] - df['true'].values.reshape(-1,1)).mean(axis=0)\n",
    "    # round to 2 decimal places\n",
    "    mae = np.round(mae, 2)\n",
    "    print(f\"Mean absolute error: \\n{mae}\")\n",
    "\n",
    "    # compute total sum of squares\n",
    "    tss = np.square(df['true'] - df['true'].mean()).sum()\n",
    "\n",
    "    # compute residual sum of squares\n",
    "    rss = np.square(df.iloc[:,1:-1] - df['true'].values.reshape(-1,1)).sum(axis=0)\n",
    "\n",
    "    # compute R2\n",
    "    r2 = 1 - rss/tss\n",
    "    # round to 2 decimal places\n",
    "    r2 = np.round(r2, 2)\n",
    "    print(f\"R2: \\n{r2}\")\n",
    "\n",
    "    # create dataframe for mae, mse and r2\n",
    "    df_metrics = pd.DataFrame({'model': ['model 1', 'model 2', 'model 3', 'model 4', 'model 5'],\n",
    "                        'mae': mae,\n",
    "                        'r2': r2})\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = [9, 6]\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "    plt.plot(df.index, df['true'].values, 'ys', color='black', linestyle='-')\n",
    "    plt.plot(df.index, df['model 1'].values, 'x', color='red', label='$MAE = $' + str(mae[0]), linestyle='-')\n",
    "    plt.plot(df.index, df['model 2'].values, 'x', color='green', label='$MAE = $' + str(mae[1]), linestyle='-')\n",
    "    plt.plot(df.index, df['model 3'].values, 'x', color='blue', label='$MAE = $' + str(mae[2]), linestyle='-')\n",
    "    plt.plot(df.index, df['model 4'].values, 'x', color='purple', label='$MAE = $' + str(mae[3]), linestyle='-')\n",
    "    plt.plot(df.index, df['model 5'].values, 'x', color='darkorange', label='$MAE = $' + str(mae[4]), linestyle='-')\n",
    "    plt.ylim(bottom=0, top=100)\n",
    "    plt.ylabel('PDX1/NKX6.1 double positives (%)')\n",
    "    plt.xticks(df.index, df['sample'].values, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.savefig(f'd13_pred_testfold{test_round+1}.png')\n",
    "    plt.show()\n",
    "\n",
    "    colors = ['red', 'green', 'blue', 'purple', 'darkorange']\n",
    "    for i in range(5):\n",
    "        plt.rcParams['figure.figsize'] = [9, 6]\n",
    "        # Plot predictions\n",
    "        column = 'model ' + str(i+1)\n",
    "        plt.plot(df.index, df['true'].values, 'ys', color='black', linestyle='-', label='true values')\n",
    "        plt.plot(df.index, df[column].values, 'x', color=colors[i], label='predicted values', linestyle='-')\n",
    "        plt.fill_between(df.index, df['true'], df[column], color=colors[i], alpha=0.3)\n",
    "        plt.bar(df.index, np.abs(df[column] - df['true']), color=colors[i], alpha=0.3, label='absolute residuals')\n",
    "        plt.plot(df.index, mae[i]*np.ones(df.shape[0]), color=colors[i], linestyle='--', label='mean absolute error')\n",
    "        plt.text(df.index[0], mae[i] + 2, str(mae[i]), fontsize=16, ha='center')\n",
    "        plt.ylim(bottom=0, top=100)\n",
    "        plt.ylabel('PDX1/NKX6.1 double positives (%)')\n",
    "        plt.xticks(df.index, df['sample'].values, rotation=45)\n",
    "        plt.yticks(np.arange(0, 101, 10))\n",
    "        plt.legend(fontsize=14)\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'd13_pred_testfold{test_round+1}_model{i+1}_mae.png')\n",
    "        plt.show()\n",
    "\n",
    "        plt.scatter(df['true'], df[column], color=colors[i], label='$MAE = $' + str(mae[i]))\n",
    "        plt.plot([0, 100], [0, 100], 'k--')\n",
    "        plt.xlabel('true')\n",
    "        plt.ylabel('predicted')\n",
    "        ax = plt.gca()\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        plt.legend(fontsize=14)\n",
    "        plt.savefig(f'd13_pred_testfold{test_round+1}_model{i+1}_scatter.png')\n",
    "        plt.show()\n",
    "    \n",
    "    # save predictions\n",
    "    df.to_csv(f'd13_pred_testfold{test_round+1}.csv', sep=';')\n",
    "    df_metrics.to_csv(f'd13_metrics_testfold{test_round+1}.csv', sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
