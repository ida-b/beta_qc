{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 1: import functions #####\n",
    "from keras.layers import Dense, Flatten, BatchNormalization, Activation, Conv2D, Conv1D, AveragePooling2D, AveragePooling1D, Input, MaxPooling1D, Dropout\n",
    "from keras.models import load_model, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "from keras import backend as K, initializers, regularizers\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from numpy.random import seed; seed(473)\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "# from tensorflow import set_random_seed; set_random_seed(763)\n",
    "from keras.losses import binary_crossentropy\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "# from sklearn.externals.six import StringIO  \n",
    "from sklearn.tree import export_graphviz, DecisionTreeRegressor\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.utils import class_weight\n",
    "from scipy.stats import ttest_ind\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 2: load data #####\n",
    "x = np.load('supermatrix_unfiltered_all_log_clipped_0-1_norm1.npy')\n",
    "metaData = pd.read_csv('data/supermatrix_labels.csv', delimiter=';')\n",
    "y = metaData['D4 pass'].values\n",
    "\n",
    "print('Total number of samples: ', x.shape[0])\n",
    "print('Number of positive samples (D4 pass): ', np.sum(y))\n",
    "print('Number of negative samples (D4 fail): ', y.shape[0]-np.sum(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test and train folds\n",
    "# test_folds = np.load('data/test_folds.npy', allow_pickle=True)\n",
    "test_folds = np.load('data/test_folds_super.npy', allow_pickle=True)\n",
    "train_folds = np.load('data/train_folds_super.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Define test and train folds #####\n",
    "samples = metaData['sample'].values\n",
    "\n",
    "# find all samples labelled as D4 pass\n",
    "d4_pass = metaData[metaData['D4 pass']==1]['sample'].values\n",
    "d4_fail = metaData[metaData['D4 pass']==0]['sample'].values\n",
    "\n",
    "# shuffle data and split into eight folds\n",
    "random.shuffle(d4_pass)\n",
    "random.shuffle(d4_fail)\n",
    "test_folds_pass = np.array_split(d4_pass, 7)\n",
    "test_folds_fail = np.array_split(d4_fail, 7)\n",
    "\n",
    "# concatenate test folds\n",
    "test_folds = []\n",
    "for i in range(7):\n",
    "    test_folds.append(np.concatenate((test_folds_pass[i], test_folds_fail[6-i])))\n",
    "\n",
    "# create 5-fold train-validation splits for each test fold\n",
    "train_folds = []\n",
    "for test_fold in test_folds:\n",
    "    idx_train = np.invert(np.isin(samples, test_fold))\n",
    "    idx_test = np.isin(samples, test_fold)\n",
    "\n",
    "    samples_train = samples[idx_train]\n",
    "    random.shuffle(samples_train)\n",
    "    train_fold = np.array_split(samples_train, 5)\n",
    "\n",
    "    train_folds.append(train_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 4: define model #####\n",
    "\n",
    "def create_model(x, l2=0.001, dropout=1/3):\n",
    "    # input\n",
    "    model_input = Input(shape=x[0].shape)\n",
    "\n",
    "    # first convolution layer\n",
    "    model_output = Conv1D(4, \n",
    "                        kernel_size=1, \n",
    "                        kernel_initializer=initializers.RandomUniform(),\n",
    "                        kernel_regularizer=regularizers.l2(l2),\n",
    "                        activation=None)(model_input)\n",
    "    model_output = BatchNormalization()(model_output)\n",
    "    model_output = Activation(\"relu\")(model_output)\n",
    "\n",
    "    # second convolution layer\n",
    "    model_output = Conv1D(4,\n",
    "                        kernel_size=1,\n",
    "                        kernel_initializer=initializers.RandomUniform(),\n",
    "                        kernel_regularizer=regularizers.l2(l2),\n",
    "                        activation=None)(model_output)\n",
    "    model_output = BatchNormalization()(model_output)\n",
    "    model_output = Activation(\"relu\")(model_output)\n",
    "\n",
    "    # pooling layer\n",
    "    model_output = AveragePooling1D(pool_size=x.shape[1])(model_output)\n",
    "    model_output = Flatten()(model_output)\n",
    "\n",
    "    # # dropout\n",
    "    # model_output = Dropout(rate=1/4)(model_output)\n",
    "\n",
    "    # Dense layer\n",
    "    model_output = Dense(3, \n",
    "                        kernel_initializer=initializers.RandomUniform(),\n",
    "                        kernel_regularizer=regularizers.l2(l2),\n",
    "                        activation=None)(model_output)\n",
    "    model_output = BatchNormalization()(model_output)\n",
    "    model_output = Activation(\"relu\")(model_output)\n",
    "\n",
    "    # output layer\n",
    "    model_output = Dense(1, \n",
    "                        kernel_initializer=initializers.RandomUniform(),\n",
    "                        activation=None)(model_output)\n",
    "    model_output = BatchNormalization()(model_output)\n",
    "    model_output = Activation(\"sigmoid\")(model_output)\n",
    "\n",
    "    return Model(inputs=model_input, outputs=model_output)\n",
    "\n",
    "# define function for plotting train and validation loss and accuracy\n",
    "def plot_loss_acc(history):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    # increase font size\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    # increase axes font size\n",
    "    axes[0].tick_params(axis='both', which='major', labelsize=14)\n",
    "    axes[1].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "    axes[0].plot(history.history['loss'])\n",
    "    axes[0].plot(history.history['val_loss'])\n",
    "    axes[0].set_title('train vs validation loss')\n",
    "    axes[0].set_ylabel('loss')\n",
    "    axes[0].set_xlabel('epoch')\n",
    "    axes[0].legend(['train', 'validation'], loc='best')\n",
    "\n",
    "    axes[1].plot(history.history['acc'])\n",
    "    axes[1].plot(history.history['val_acc'])\n",
    "    axes[1].set_title('train vs validation accuracy')\n",
    "    axes[1].set_ylabel('accuracy')\n",
    "    axes[1].set_xlabel('epoch')\n",
    "    axes[1].legend(['train', 'validation'], loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def plot_prec_rec_f1(history):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(22, 6))\n",
    "    # increase font size\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    # increase axes font size\n",
    "    axes[0].tick_params(axis='both', which='major', labelsize=14)\n",
    "    axes[1].tick_params(axis='both', which='major', labelsize=14)\n",
    "    axes[2].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "    axes[0].plot(history.history['precision'])\n",
    "    axes[0].plot(history.history['val_precision'])\n",
    "    axes[0].set_title('train vs validation precision')\n",
    "    axes[0].set_ylabel('precision')\n",
    "    axes[0].set_xlabel('epoch')\n",
    "    axes[0].legend(['train', 'validation'], loc='best')\n",
    "\n",
    "    axes[1].plot(history.history['recall'])\n",
    "    axes[1].plot(history.history['val_recall'])\n",
    "    axes[1].set_title('train vs validation recall')\n",
    "    axes[1].set_ylabel('recall')\n",
    "    axes[1].set_xlabel('epoch')\n",
    "    axes[1].legend(['train', 'validation'], loc='best')\n",
    "\n",
    "    axes[2].plot(history.history['f1'])\n",
    "    axes[2].plot(history.history['val_f1'])\n",
    "    axes[2].set_title('train vs validation f1')\n",
    "    axes[2].set_ylabel('f1')\n",
    "    axes[2].set_xlabel('epoch')\n",
    "    axes[2].legend(['train', 'validation'], loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred, class_weights):\n",
    "    # Weighted loss for class 0 and class 1\n",
    "    w0 = class_weights[0]\n",
    "    w1 = class_weights[1]\n",
    "    \n",
    "    # Calculate the weighted binary cross-entropy\n",
    "    bce = binary_crossentropy(y_true, y_pred)\n",
    "    weights = (1-y_true)*w0 + y_true*w1\n",
    "    weighted_loss = bce*weights\n",
    "    return weighted_loss\n",
    "\n",
    "# define custom metrics: precision, recall, f1\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    total_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    \n",
    "    recall = true_positives / (total_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    \"\"\"F1 metric.\n",
    "    Only computes a batch-wise average of f1.\n",
    "    Computes the f1, a metric for multi-label classification of\n",
    "    how many relevant items are selected and how many selected items are relevant.\n",
    "    \"\"\"\n",
    "\n",
    "    precision_value = precision(y_true, y_pred)\n",
    "    recall_value = recall(y_true, y_pred)\n",
    "    \n",
    "    f1 = 2*((precision_value*recall_value)/(precision_value+recall_value+K.epsilon()))\n",
    "    return f1\n",
    "\n",
    "def compute_class_weights(y):\n",
    "    \"\"\"Compute class weights for unbalanced datasets.\"\"\"\n",
    "    class_weights = {0: len(y)/(len(y)-np.sum(y)), 1: 1}\n",
    "    # class_weights = {0: len(y)/(len(y)-np.sum(y)), 1: len(y)/np.sum(y)}\n",
    "    return class_weights\n",
    "\n",
    "def compute_inverse_frequency_weights_binary(y):\n",
    "    # Calculate the frequency of class 0 and class 1\n",
    "    count_class_0 = np.sum(y == 0)\n",
    "    count_class_1 = np.sum(y == 1)\n",
    "\n",
    "    # Calculate the total number of samples\n",
    "    total_samples = len(y)\n",
    "\n",
    "    # Calculate the inverse frequency class weights\n",
    "    weight_class_0 = total_samples/(count_class_0*2)  # Class 0 weight\n",
    "    weight_class_1 = total_samples/(count_class_1*2)  # Class 1 weight\n",
    "\n",
    "    class_weights = {0: weight_class_0, 1: weight_class_1}\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "def compute_sqrt_inverse_frequency_weights_binary(y):\n",
    "    # Calculate the frequency of class 0 and class 1\n",
    "    count_class_0 = np.sum(y == 0)\n",
    "    count_class_1 = np.sum(y == 1)\n",
    "\n",
    "    # Calculate the total number of samples\n",
    "    total_samples = len(y)\n",
    "\n",
    "    # Calculate the square root of inverse frequency class weights\n",
    "    weight_class_0 = np.sqrt(total_samples/(count_class_0*2))  # Class 0 weight\n",
    "    weight_class_1 = np.sqrt(total_samples/(count_class_1*2))  # Class 1 weight\n",
    "\n",
    "    class_weights = {0: weight_class_0, 1: weight_class_1}\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "def compute_logarithmic_weights_binary(y):\n",
    "    # Calculate the frequency of class 0 and class 1\n",
    "    count_class_0 = np.sum(y == 0)\n",
    "    count_class_1 = np.sum(y == 1)\n",
    "\n",
    "    # Calculate the total number of samples\n",
    "    total_samples = len(y)\n",
    "\n",
    "    # Calculate the logarithmic class weights (adding 1 to avoid division by zero)\n",
    "    weight_class_0 = math.log(total_samples/(count_class_0 * 2) + 1)  # Class 0 weight\n",
    "    weight_class_1 = math.log(total_samples/(count_class_1 * 2) + 1)  # Class 1 weight\n",
    "\n",
    "    class_weights = {0: weight_class_0, 1: weight_class_1}\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "def calc_mean_std(history):\n",
    "    mean = np.array(history).mean(axis=0)\n",
    "    std = np.array(history).std(axis=0)\n",
    "    return mean, std\n",
    "\n",
    "def plot_error_bar(train_mean, train_std, val_mean, val_std, ylabel, ax, leg_loc='best'):\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax.errorbar(range(1, len(train_mean) + 1), train_mean, yerr=train_std, label='train')\n",
    "    ax.errorbar(range(1, len(val_mean) + 1), val_mean, yerr=val_std, label='validation')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend(loc=leg_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 5: train and test model #####\n",
    "samples = metaData['sample'].values\n",
    "for test_round, test_fold in enumerate(test_folds):\n",
    "    print(f\"Test fold {test_round+1}/{len(test_folds)}\")\n",
    "    \n",
    "    idx_test = np.isin(samples, test_fold)\n",
    "    idx_train = np.invert(idx_test)\n",
    "\n",
    "    # choose channels that are to be included in the analysis\n",
    "    # channels = [0,3,6,9,12,15,18,21,24,27,30,33,36,39,42,45]\n",
    "    # channels = [0,3,24]\n",
    "\n",
    "    # x_train = x[idx_train,:,:]; x_train = x_train[:,:,channels]; y_train = y[idx_train]\n",
    "    # x_test = x[idx_test,:,:]; x_test = x_test[:,:,channels]; y_test = y[idx_test]\n",
    "\n",
    "    x_train = x[idx_train,:,:]; y_train = y[idx_train]; samples_train = samples[idx_train]\n",
    "    x_test = x[idx_test,:,:]; y_test = y[idx_test]; samples_test = samples[idx_test]\n",
    "\n",
    "    # # k-fold cross-validation\n",
    "    # # Specify the number of folds for cross-validation\n",
    "    # num_folds = 5\n",
    "\n",
    "    # # Create a KFold instance\n",
    "    # # generate random number between 0 and 1000\n",
    "    # seed = random.randint(0, 1000)\n",
    "    # kf = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Lists to store evaluation results\n",
    "    val_loss_history = []; val_accuracy_history = []; val_recall_history = []; val_precision_history = []; val_f1_history = []\n",
    "    train_loss_history = []; train_accuracy_history = []; train_recall_history = []; train_precision_history = []; train_f1_history = []\n",
    "\n",
    "    class_weights_folds = []\n",
    "    acc_best_epoch = []; loss_best_epoch = []\n",
    "\n",
    "    # for fold, (train_indices, valid_indices) in enumerate(kf.split(x_train)):\n",
    "    for fold, valid_samples in enumerate(train_folds[test_round]):\n",
    "        print(f\"Training fold {fold+1}/{len(train_folds[test_round])}\")\n",
    "\n",
    "        valid_indices = np.isin(samples_train, valid_samples)\n",
    "        train_indices = np.invert(valid_indices)\n",
    "        \n",
    "        x_train_fold = x_train[train_indices]\n",
    "        y_train_fold = y_train[train_indices]\n",
    "        x_valid_fold = x_train[valid_indices]\n",
    "        y_valid_fold = y_train[valid_indices]\n",
    "\n",
    "        class_weights = compute_class_weights(y_train_fold)\n",
    "        class_weights_folds.append(class_weights)\n",
    "        print(f\"Class weights: {class_weights}\")\n",
    "        \n",
    "        # Build the model (same as your previous code)\n",
    "        model = create_model(x_train, l2=0.01)\n",
    "        model.compile(loss=lambda y_true, y_pred: weighted_binary_crossentropy(y_true, y_pred, class_weights),\n",
    "                    optimizer=Adam(lr=0.0001),\n",
    "                    metrics=['accuracy', precision, recall, f1])\n",
    "        \n",
    "        checkpointer = ModelCheckpoint(filepath=f'saved_weights_unfiltered_super_testfold{test_round+1}_fold{fold+1}.hdf5', \n",
    "                                    monitor='val_loss', verbose=0, \n",
    "                                    save_best_only=True)\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(x_train_fold, y_train_fold,\n",
    "                            batch_size=5,\n",
    "                            epochs=300, \n",
    "                            verbose=0,\n",
    "                            callbacks=[checkpointer],\n",
    "                            validation_data=([x_valid_fold], y_valid_fold),\n",
    "                            class_weight=class_weights)\n",
    "        \n",
    "        plot_loss_acc(model.history)\n",
    "        plot_prec_rec_f1(model.history)\n",
    "        \n",
    "        # Store validation metrics for each fold\n",
    "        val_loss_history.append(history.history['val_loss'])\n",
    "        val_accuracy_history.append(history.history['val_acc'])\n",
    "        val_recall_history.append(history.history['val_recall'])\n",
    "        val_precision_history.append(history.history['val_precision'])\n",
    "        val_f1_history.append(history.history['val_f1'])\n",
    "        train_loss_history.append(history.history['loss'])\n",
    "        train_accuracy_history.append(history.history['acc'])\n",
    "        train_recall_history.append(history.history['recall'])\n",
    "        train_precision_history.append(history.history['precision'])\n",
    "        train_f1_history.append(history.history['f1'])\n",
    "\n",
    "        # Find the epoch with the lowest validation loss\n",
    "        best_epoch = np.argmin(history.history['val_loss'])\n",
    "        acc_best_epoch.append(history.history['val_acc'][best_epoch])\n",
    "        loss_best_epoch.append(history.history['val_loss'][best_epoch])\n",
    "\n",
    "        # print validation accuracy and loss at best epoch\n",
    "        print(f\"Validation accuracy at best epoch: {history.history['val_acc'][best_epoch]:.2f}\")\n",
    "        print(f\"Validation loss at best epoch: {history.history['val_loss'][best_epoch]:.2f}\")\n",
    "    \n",
    "    # print model with lowest validation loss and highest validation accuracy\n",
    "    print(f\"Model with lowest validation loss at best epoch: {np.argmin(loss_best_epoch)+1}\")\n",
    "    print(f\"Model with highest validation accuracy at best epoch: {np.argmax(acc_best_epoch)+1}\")\n",
    "\n",
    "    # Calculate average validation loss and accuracy for each model\n",
    "    avg_val_loss = np.array(val_loss_history).mean(axis=1)\n",
    "    avg_val_accuracy = np.array(val_accuracy_history).mean(axis=1)\n",
    "    avg_val_f1 = np.array(val_f1_history).mean(axis=1)\n",
    "\n",
    "    # Choose the model with the highest average validation accuracy\n",
    "    print(f\"Model with lowest average validation loss: {np.argmin(avg_val_loss)+1}\")\n",
    "    print(f\"Model with highest average validation accuracy: {np.argmax(avg_val_accuracy)+1}\")\n",
    "    print(f\"Model with highest average validation f1: {np.argmax(avg_val_f1)+1}\")\n",
    "\n",
    "    # plot average validation (train, validation) and accuracy (train, validation) vs. epoch with error bars\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "    # loss\n",
    "    [train_mean, train_std] = calc_mean_std(train_loss_history)\n",
    "    [val_mean, val_std] = calc_mean_std(val_loss_history)\n",
    "    print(f\"Average train loss: {train_mean[-1]:.2f} +/- {train_std[-1]:.2f}\")\n",
    "    print(f\"Average validation loss: {val_mean[-1]:.2f} +/- {val_std[-1]:.2f}\")\n",
    "\n",
    "    plot_error_bar(train_mean, train_std, val_mean, val_std, 'loss', axes[0], leg_loc='upper right')\n",
    "\n",
    "    # accuracy\n",
    "    [train_mean, train_std] = calc_mean_std(train_accuracy_history)\n",
    "    [val_mean, val_std] = calc_mean_std(val_accuracy_history)\n",
    "    print(f\"Average train accuracy: {train_mean[-1]:.2f} +/- {train_std[-1]:.2f}\")\n",
    "    print(f\"Average validation accuracy: {val_mean[-1]:.2f} +/- {val_std[-1]:.2f}\")\n",
    "\n",
    "    plot_error_bar(train_mean, train_std, val_mean, val_std, 'accuracy', axes[1], leg_loc='lower right')\n",
    "\n",
    "    plt.suptitle('5-fold cross-validation')\n",
    "    plt.show()\n",
    "        \n",
    "    # plot average precision (train, validation), recall (train, validation) and f1 (train, validation) vs. epoch with error bars\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(22, 6))\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "    # precision\n",
    "    [train_mean, train_std] = calc_mean_std(train_precision_history)\n",
    "    [val_mean, val_std] = calc_mean_std(val_precision_history)\n",
    "    print(f\"Average train precision: {train_mean[-1]:.2f} +/- {train_std[-1]:.2f}\")\n",
    "    print(f\"Average validation precision: {val_mean[-1]:.2f} +/- {val_std[-1]:.2f}\")\n",
    "\n",
    "    plot_error_bar(train_mean, train_std, val_mean, val_std, 'precision', axes[0], leg_loc='lower right')\n",
    "\n",
    "    # recall\n",
    "    [train_mean, train_std] = calc_mean_std(train_recall_history)\n",
    "    [val_mean, val_std] = calc_mean_std(val_recall_history)\n",
    "    print(f\"Average train recall: {train_mean[-1]:.2f} +/- {train_std[-1]:.2f}\")\n",
    "    print(f\"Average validation recall: {val_mean[-1]:.2f} +/- {val_std[-1]:.2f}\")\n",
    "\n",
    "    plot_error_bar(train_mean, train_std, val_mean, val_std, 'recall', axes[1], leg_loc='lower right')\n",
    "\n",
    "    # f1\n",
    "    [train_mean, train_std] = calc_mean_std(train_f1_history)\n",
    "    [val_mean, val_std] = calc_mean_std(val_f1_history)\n",
    "    print(f\"Average train f1: {train_mean[-1]:.2f} +/- {train_std[-1]:.2f}\")\n",
    "    print(f\"Average validation f1: {val_mean[-1]:.2f} +/- {val_std[-1]:.2f}\")\n",
    "\n",
    "    plot_error_bar(train_mean, train_std, val_mean, val_std, 'f1', axes[2], leg_loc='lower right')\n",
    "\n",
    "    plt.suptitle('5-fold cross validation')\n",
    "    plt.show()\n",
    "\n",
    "    # test models on test fold\n",
    "    y_5fold_scores = np.zeros((y_test.shape[0], 5))\n",
    "\n",
    "    for i in range(5):\n",
    "        final_model = load_model(f'saved_weights_unfiltered_super_testfold{test_round+1}_fold{i+1}.hdf5', compile=False)\n",
    "\n",
    "        # define loss function and optimizer\n",
    "        final_model.compile(loss=lambda y_true, y_pred: weighted_binary_crossentropy(y_true, y_pred, class_weights_folds[i]),\n",
    "                            optimizer=Adam(lr=0.0001),\n",
    "                            metrics=['accuracy', precision, recall, f1])\n",
    "\n",
    "        # generate ROC and AUC\n",
    "        y_scores = final_model.predict([x_test])\n",
    "        y_5fold_scores[:,i] = y_scores.flatten()\n",
    "\n",
    "        # test accuracy\n",
    "        y_pred = np.rint(y_scores)\n",
    "        accuracy = np.sum(1-np.abs(y_pred.reshape(-1)-y_test))/len(y_test)\n",
    "        print(f\"Accuracy on test fold {test_round+1} and train fold {i+1}: {accuracy:.2f}\")\n",
    "\n",
    "    y_mean_scores = np.mean(y_5fold_scores, axis=1)\n",
    "\n",
    "    # generate ROC and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_mean_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # test accuracy\n",
    "    y_pred = np.rint(y_mean_scores)\n",
    "    accuracy = np.sum(1-np.abs(y_pred.reshape(-1)-y_test))/len(y_test)\n",
    "\n",
    "    # print prediction and true label\n",
    "    df = pd.DataFrame({'sample': samples[idx_test],\n",
    "                        'fold1_pred': y_5fold_scores[:,0],\n",
    "                        'fold2_pred': y_5fold_scores[:,1],\n",
    "                        'fold3_pred': y_5fold_scores[:,2],\n",
    "                        'fold4_pred': y_5fold_scores[:,3],\n",
    "                        'fold5_pred': y_5fold_scores[:,4],\n",
    "                        'mean_pred': y_mean_scores.flatten(),\n",
    "                        'true': y_test})\n",
    "    print(df)\n",
    "    \n",
    "    # save predictions\n",
    "    df.to_csv(f'predictions_unfiltered_testfold{test_round+1}.csv', sep=';')\n",
    "\n",
    "    # fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n",
    "\n",
    "    # # plot ROC curve\n",
    "    # axes[0].plot(fpr, tpr)\n",
    "    # axes[0].plot([0, 1], [0, 1], 'k--')\n",
    "    # axes[0].set_xlabel('False positive rate')\n",
    "    # axes[0].set_ylabel('True positive rate')\n",
    "    # axes[0].set_title('AUC = {0:.2f}'.format(roc_auc))\n",
    "\n",
    "    # # Calculate reliability curve\n",
    "    # prob_true, prob_pred = calibration_curve(y_test, y_mean_scores, n_bins=10, strategy='uniform')\n",
    "\n",
    "    # axes[1].plot(prob_pred, prob_true, marker='o', label='Model Calibration')\n",
    "    # axes[1].plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n",
    "    # axes[1].set_xlabel('Mean predicted probability')\n",
    "    # axes[1].set_ylabel('Fraction of positives')\n",
    "    # axes[1].set_title('Probability calibration plot')\n",
    "    # axes[1].legend()\n",
    "\n",
    "    # # Create the scatter plot\n",
    "    # hb = plt.hexbin(y_mean_scores.flatten(), y_test, gridsize=15, cmap='coolwarm', alpha=0.5, extent=[0,1,0,1])\n",
    "    # plt.colorbar(hb, label='Density')\n",
    "    # axes[2].set_xlabel('Predicted score')\n",
    "    # axes[2].set_ylabel('True label')\n",
    "    # axes[2].set_title('Predicted scores vs true labels')\n",
    "    # axes[2].set_xlim(-0.1, 1.1)  # Set x-axis limit from 0 to 1 (probability range)\n",
    "    # axes[2].set_ylim(-0.1, 1.1)  # Set y-axis limit to accommodate binary labels\n",
    "    # axes[2].grid(True)\n",
    "\n",
    "    # plt.suptitle(f'Average accuracy = {round(accuracy*100,1)}%')\n",
    "    # plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    # plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
