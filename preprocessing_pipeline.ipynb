{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FlowCal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to fcs files\n",
    "path = 'fox'\n",
    "\n",
    "# create empty lists for storing data\n",
    "data_norm1 = []; data_norm2 = []\n",
    "data_stdrd1 = []; data_stdrd2 = []\n",
    "samples = []\n",
    "\n",
    "# channels_fox_sox = ['FSC-A','SSC-A','FL1-A','FL7-A']\n",
    "# channels_oct = ['FSC-A','SSC-A','FL1-A','FL13-A']\n",
    "channels = ['FSC-A','SSC-A','FL1-A','FL2-A','FL3-A','FL4-A','FL5-A','FL6-A','FL7-A','FL8-A','FL9-A','FL10-A','FL11-A','FL12-A','FL13-A','FL14-A']\n",
    "\n",
    "ctrl_sox = 0\n",
    "ctrl_oct = 0\n",
    "\n",
    "# load files\n",
    "for fox_file in os.listdir(path):\n",
    "    fox = FlowCal.io.FCSData(path + '/' + fox_file)\n",
    "    fox = fox[:,channels]\n",
    "\n",
    "    # find corresponding single cell masks in folder \"robust_clust_fox/singlets masks\"\n",
    "    exp = fox_file.split(' DE')[0]\n",
    "    if len(exp.split(' ')) > 1:\n",
    "        exp = exp.split(' ')[1]\n",
    "\n",
    "    if len(fox) < 2000:\n",
    "        print('less than 2000 events in fox')\n",
    "        continue\n",
    "\n",
    "    for sox_file in os.listdir('sox'):\n",
    "        if exp in sox_file:\n",
    "            sox = FlowCal.io.FCSData('sox/' + sox_file)\n",
    "            sox = sox[:,channels]\n",
    "            ctrl_sox = 1\n",
    "            break\n",
    "    \n",
    "    # if no sox file, skip\n",
    "    if ctrl_sox == 0:\n",
    "        print('no sox file')\n",
    "        continue\n",
    "    ctrl_sox = 0\n",
    "\n",
    "    if len(sox) < 2000:\n",
    "        print('less than 2000 events in sox')\n",
    "        continue\n",
    "\n",
    "    for oct_file in os.listdir('oct'):\n",
    "        if exp in oct_file:\n",
    "            oct = FlowCal.io.FCSData('oct/' + oct_file)\n",
    "            oct = oct[:,channels]\n",
    "            ctrl_oct = 1\n",
    "            break\n",
    "    \n",
    "    # if no oct file, skip\n",
    "    if ctrl_oct == 0:\n",
    "        print('no oct file')\n",
    "        continue\n",
    "    ctrl_oct = 0\n",
    "\n",
    "    if len(oct) < 2000:\n",
    "        print('less than 2000 events in oct')\n",
    "        continue\n",
    "\n",
    "    # apply log transform\n",
    "    fox[fox < 0] = 0; fox = np.log10(fox+1)\n",
    "    sox[sox < 0] = 0; sox = np.log10(sox+1)\n",
    "    oct[oct < 0] = 0; oct = np.log10(oct+1)\n",
    "\n",
    "    # fox_min = np.min(fox, axis=0); fox_min[fox_min > 0] = 0; fox = fox - fox_min; fox[:,2:] = np.log10(fox[:,2:]+1)\n",
    "    # sox_min = np.min(sox, axis=0); sox_min[sox_min > 0] = 0; sox = sox - sox_min; sox[:,2:] = np.log10(sox[:,2:]+1)\n",
    "    # oct_min = np.min(oct, axis=0); oct_min[oct_min > 0] = 0; oct = oct - oct_min; oct[:,2:] = np.log10(oct[:,2:]+1)\n",
    "\n",
    "    # clip to [0, 0.95] quantile\n",
    "    quantiles = np.quantile(fox, 0.85, axis=0); fox_clipped = np.clip(fox, 0, quantiles)\n",
    "    quantiles = np.quantile(sox, 0.85, axis=0); sox_clipped = np.clip(sox, 0, quantiles)\n",
    "    quantiles = np.quantile(oct, 0.85, axis=0); oct_clipped = np.clip(oct, 0, quantiles)\n",
    "    # fox_clipped = np.clip(fox, 0, quantiles[0])\n",
    "    # sox_clipped = np.clip(sox, 0, quantiles[1])\n",
    "    # oct_clipped = np.clip(oct, 0, quantiles[2])\n",
    "\n",
    "    # apply min-max normalization\n",
    "    fox_norm = MinMaxScaler().fit_transform(fox_clipped)\n",
    "    sox_norm = MinMaxScaler().fit_transform(sox_clipped)\n",
    "    oct_norm = MinMaxScaler().fit_transform(oct_clipped)\n",
    "    # fox_norm = (fox_clipped - extrema[0,0]) / (extrema[0,1] - extrema[0,0])\n",
    "    # sox_norm = (sox_clipped - extrema[1,0]) / (extrema[1,1] - extrema[1,0])\n",
    "    # oct_norm = (oct_clipped - extrema[2,0]) / (extrema[2,1] - extrema[2,0])\n",
    "\n",
    "    # apply standardization\n",
    "    fox_stdrd = StandardScaler().fit_transform(fox)\n",
    "    sox_stdrd = StandardScaler().fit_transform(sox)\n",
    "    oct_stdrd = StandardScaler().fit_transform(oct)\n",
    "    # fox_stdrd = (fox - stats[0,0]) / stats[0,1]\n",
    "    # sox_stdrd = (sox - stats[1,0]) / stats[1,1]\n",
    "    # oct_stdrd = (oct - stats[2,0]) / stats[2,1]\n",
    "    # fox_stdrd = (fox_clipped - stats[0,0]) / stats[0,1]\n",
    "    # sox_stdrd = (sox_clipped - stats[1,0]) / stats[1,1]\n",
    "    # oct_stdrd = (oct_clipped - stats[2,0]) / stats[2,1]\n",
    "\n",
    "    # randomly select 1000 events\n",
    "    order = np.arange(len(fox)); np.random.shuffle(order); fox_norm = fox_norm[order[:2000],:]; fox_stdrd = fox_stdrd[order[:2000],:]\n",
    "    order = np.arange(len(sox)); np.random.shuffle(order); sox_norm = sox_norm[order[:2000],:]; sox_stdrd = sox_stdrd[order[:2000],:]\n",
    "    order = np.arange(len(oct)); np.random.shuffle(order); oct_norm = oct_norm[order[:2000],:]; oct_stdrd = oct_stdrd[order[:2000],:]\n",
    "\n",
    "    # build matrix of shape fox_norm.shape[0] + sox_norm.shape[0] + oct_norm.shape[0], fox_norm.shape[1] + sox_norm.shape[1] + oct_norm.shape[1]\n",
    "    # fill with -1\n",
    "    # fill diagonal blocks with fox_norm, sox_norm, oct_norm\n",
    "    # fill off-diagonal blocks with -1\n",
    "    # super_norm1 = np.zeros((6000, 12)); super_norm1.fill(-1)\n",
    "    # super_norm1[:fox_norm.shape[0],:fox_norm.shape[1]] = fox_norm\n",
    "    # super_norm1[fox_norm.shape[0]:fox_norm.shape[0]+sox_norm.shape[0],fox_norm.shape[1]:fox_norm.shape[1]+sox_norm.shape[1]] = sox_norm\n",
    "    # super_norm1[fox_norm.shape[0]+sox_norm.shape[0]:,fox_norm.shape[1]+sox_norm.shape[1]:] = oct_norm\n",
    "\n",
    "    # super_norm2 = np.zeros((6000, 5)); super_norm2.fill(-1)\n",
    "    # super_norm2[:fox_norm.shape[0],:fox_norm.shape[1]] = fox_norm\n",
    "    # super_norm2[fox_norm.shape[0]:fox_norm.shape[0]+sox_norm.shape[0],:sox_norm.shape[1]] = sox_norm\n",
    "    # super_norm2[fox_norm.shape[0]+sox_norm.shape[0]:,:oct_norm.shape[1]-1] = oct_norm[:,:-1]\n",
    "    # super_norm2[fox_norm.shape[0]+sox_norm.shape[0]:,-1] = oct_norm[:,-1]\n",
    "    super_norm1 = np.zeros((6000, len(channels)*3)); super_norm1.fill(-1)\n",
    "    super_norm1[:fox_norm.shape[0],:fox_norm.shape[1]] = fox_norm\n",
    "    super_norm1[fox_norm.shape[0]:fox_norm.shape[0]+sox_norm.shape[0],fox_norm.shape[1]:fox_norm.shape[1]+sox_norm.shape[1]] = sox_norm\n",
    "    super_norm1[fox_norm.shape[0]+sox_norm.shape[0]:,fox_norm.shape[1]+sox_norm.shape[1]:] = oct_norm\n",
    "\n",
    "    # super_norm2 = np.zeros((6000, len(channels))); super_norm2.fill(-1)\n",
    "    # super_norm2[:fox_norm.shape[0],:] = fox_norm\n",
    "    # super_norm2[fox_norm.shape[0]:fox_norm.shape[0]+sox_norm.shape[0],:] = sox_norm\n",
    "    # super_norm2[fox_norm.shape[0]+sox_norm.shape[0]:,:] = oct_norm\n",
    "\n",
    "    # super_stdrd1 = np.zeros((6000, 12))\n",
    "    # super_stdrd1[:fox_stdrd.shape[0],:fox_stdrd.shape[1]] = fox_stdrd\n",
    "    # super_stdrd1[fox_stdrd.shape[0]:fox_stdrd.shape[0]+sox_stdrd.shape[0],fox_stdrd.shape[1]:fox_stdrd.shape[1]+sox_stdrd.shape[1]] = sox_stdrd\n",
    "    # super_stdrd1[fox_stdrd.shape[0]+sox_stdrd.shape[0]:,fox_stdrd.shape[1]+sox_stdrd.shape[1]:] = oct_stdrd\n",
    "\n",
    "    # super_stdrd2 = np.zeros((6000, 5))\n",
    "    # super_stdrd2[:fox_stdrd.shape[0],:fox_stdrd.shape[1]] = fox_stdrd\n",
    "    # super_stdrd2[fox_stdrd.shape[0]:fox_stdrd.shape[0]+sox_stdrd.shape[0],:sox_stdrd.shape[1]] = sox_stdrd\n",
    "    # super_stdrd2[fox_stdrd.shape[0]+sox_stdrd.shape[0]:,:oct_stdrd.shape[1]-1] = oct_stdrd[:,:-1]\n",
    "    # super_stdrd2[fox_stdrd.shape[0]+sox_stdrd.shape[0]:,-1] = oct_stdrd[:,-1]\n",
    "\n",
    "    super_stdrd1 = np.zeros((6000, len(channels)*3))\n",
    "    super_stdrd1[:fox_stdrd.shape[0],:fox_stdrd.shape[1]] = fox_stdrd\n",
    "    super_stdrd1[fox_stdrd.shape[0]:fox_stdrd.shape[0]+sox_stdrd.shape[0],fox_stdrd.shape[1]:fox_stdrd.shape[1]+sox_stdrd.shape[1]] = sox_stdrd\n",
    "    super_stdrd1[fox_stdrd.shape[0]+sox_stdrd.shape[0]:,fox_stdrd.shape[1]+sox_stdrd.shape[1]:] = oct_stdrd\n",
    "\n",
    "    # super_stdrd2 = np.zeros((6000, len(channels)))\n",
    "    # super_stdrd2[:fox_stdrd.shape[0],:] = fox_stdrd\n",
    "    # super_stdrd2[fox_stdrd.shape[0]:fox_stdrd.shape[0]+sox_stdrd.shape[0],:] = sox_stdrd\n",
    "    # super_stdrd2[fox_stdrd.shape[0]+sox_stdrd.shape[0]:,:] = oct_stdrd\n",
    "\n",
    "    # append data and labels\n",
    "    data_norm1.append(super_norm1)\n",
    "    # data_norm2.append(super_norm2)\n",
    "    data_stdrd1.append(super_stdrd1)\n",
    "    # data_stdrd2.append(super_stdrd2)\n",
    "    # data_stdrd.append(sc_stdrd)\n",
    "    samples.append(exp)\n",
    "\n",
    "    print('Finished ' + exp)\n",
    "\n",
    "# convert to numpy arrays\n",
    "data_norm1 = np.array(data_norm1)\n",
    "# data_norm2 = np.array(data_norm2)\n",
    "data_stdrd1 = np.array(data_stdrd1)\n",
    "# data_stdrd2 = np.array(data_stdrd2)\n",
    "# data_stdrd = np.array(data_stdrd)\n",
    "samples = np.array(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annotations from labels.csv\n",
    "annotations = pd.read_csv('labels.csv', delimiter=';', decimal=',')\n",
    "\n",
    "# create pandas dataframe from labels\n",
    "df = pd.DataFrame(samples, columns=['sample'])\n",
    "\n",
    "# now join annotations and labels\n",
    "df = pd.merge(df, annotations, on='sample')\n",
    "\n",
    "# replace yes/no with 1/0\n",
    "df[['D4 pass', 'D25 pass']] = df[['D4 pass', 'D25 pass']].replace({'yes': 1, 'no': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data and labels\n",
    "np.save('train-test data/supermatrix_unfiltered_all_log_clipped_0-1_norm1', data_norm1)\n",
    "# np.save('train-test data/supermatrix_unfiltered_all_log_clipped_0-1_norm2', data_norm2)\n",
    "# np.save('train-test data/supermatrix_unfiltered_all_log_stdrd1', data_stdrd1)\n",
    "# np.save('train-test data/supermatrix_unfiltered_all_log_stdrd2', data_stdrd2)\n",
    "# df.to_csv('annotations.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to fcs files\n",
    "path = 'D13 Ida'\n",
    "\n",
    "# create empty lists for storing data\n",
    "data_norm = []\n",
    "data_stdrd = []\n",
    "samples = []\n",
    "\n",
    "channels = ['FSC-A','SSC-A','FL1-A','FL2-A','FL3-A','FL4-A','FL5-A','FL6-A','FL7-A','FL8-A','FL9-A','FL10-A','FL11-A','FL12-A','FL13-A','FL14-A']\n",
    "\n",
    "ctrl_sox = 0\n",
    "ctrl_oct = 0\n",
    "\n",
    "# load files\n",
    "for file in os.listdir(path):\n",
    "    s = FlowCal.io.FCSData(path + '/' + file)\n",
    "    s = s[:,channels]\n",
    "\n",
    "    # find corresponding single cell masks in folder \"robust_clust_fox/singlets masks\"\n",
    "    exp = file.split(' D13')[0]\n",
    "\n",
    "    if len(s) < 1000:\n",
    "        print('less than 1000 events')\n",
    "        continue\n",
    "\n",
    "    # apply log transform\n",
    "    s[s < 0] = 0; s = np.log10(s+1)\n",
    "\n",
    "    # clip to [0, 0.95] quantile\n",
    "    quantiles = np.quantile(s, 0.85, axis=0); s_clipped = np.clip(s, 0, quantiles)\n",
    "\n",
    "    # apply min-max normalization\n",
    "    s_norm = MinMaxScaler().fit_transform(s_clipped)\n",
    "\n",
    "    # apply standardization\n",
    "    s_stdrd = StandardScaler().fit_transform(s)\n",
    "\n",
    "    # randomly select 1000 events\n",
    "    order = np.arange(len(s)); np.random.shuffle(order); s_norm = s_norm[order[:1000],:]; s_stdrd = s_stdrd[order[:1000],:]\n",
    "\n",
    "    # append data and labels\n",
    "    data_norm.append(s_norm)\n",
    "    data_stdrd.append(s_stdrd)\n",
    "    samples.append(exp)\n",
    "\n",
    "    print('Finished ' + exp)\n",
    "\n",
    "# convert to numpy arrays\n",
    "data_norm = np.array(data_norm)\n",
    "data_stdrd = np.array(data_stdrd)\n",
    "samples = np.array(samples)\n",
    "\n",
    "# load annotations from labels.csv\n",
    "annotations = pd.read_csv('labels.csv', delimiter=';', decimal=',')\n",
    "\n",
    "# create pandas dataframe from labels\n",
    "df = pd.DataFrame(samples, columns=['sample'])\n",
    "\n",
    "# now join annotations and labels\n",
    "df = pd.merge(df, annotations, on='sample')\n",
    "\n",
    "# replace yes/no with 1/0\n",
    "df[['D4 pass', 'D25 pass']] = df[['D4 pass', 'D25 pass']].replace({'yes': 1, 'no': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data and labels\n",
    "np.save('train-test data/d13_unfiltered_all_log_stdrd', data_stdrd)\n",
    "np.save('train-test data/d13_unfiltered_all_log_norm', data_norm)\n",
    "df.to_csv('d13_annotations.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misclassifications revisited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "# load data\n",
    "annotations = pd.read_csv('supermatrix_labels.csv', delimiter=';')\n",
    "samples = annotations['sample'].values\n",
    "\n",
    "# of each sample, take only the first 2000 events in columns 0-15\n",
    "data_fox = data_stdrd1[:,:2000,:16]\n",
    "data_sox = data_stdrd1[:,2000:4000,16:32]\n",
    "data_oct = data_stdrd1[:,4000:,32:]\n",
    "\n",
    "# multiply each entry in labels by 2000\n",
    "names = np.repeat(annotations['sample'].values, 2000, axis=0)\n",
    "labels = np.repeat(annotations['D4 pass'].values, 2000, axis=0)\n",
    "\n",
    "# apply umap\n",
    "reducer = umap.UMAP(random_state=41)\n",
    "embedding_fox = reducer.fit_transform(data_fox.reshape(-1, data_fox.shape[2]))\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "embedding_sox = reducer.fit_transform(data_sox.reshape(-1, data_sox.shape[2]))\n",
    "reducer = umap.UMAP(random_state=43)\n",
    "embedding_oct = reducer.fit_transform(data_oct.reshape(-1, data_oct.shape[2]))\n",
    "\n",
    "test_folds = np.load('test_folds_super.npy', allow_pickle=True)\n",
    "train_folds = np.load('train_folds_super.npy', allow_pickle=True)\n",
    "\n",
    "test_set = test_folds[6]\n",
    "samples_train = samples[np.isin(samples, test_set, invert=True)]\n",
    "\n",
    "palette1 = [sns.color_palette(\"Blues\")[-1], sns.color_palette(\"Blues\")[1]]\n",
    "palette2 = [sns.color_palette(\"tab10\")[3]]  \n",
    "\n",
    "for sample in samples_train:\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15,5)) \n",
    "    idx = np.isin(names, test_set, invert=True) & np.isin(names, sample, invert=True)\n",
    "    sns.scatterplot(x=embedding_fox[idx,0], y=embedding_fox[idx,1], hue=labels[idx], palette=palette1, s=1.5, ax=ax[0])\n",
    "    sns.scatterplot(x=embedding_fox[names == sample,0], y=embedding_fox[names == sample,1], hue=names[names == sample], palette=['red'], s=1.5, ax=ax[0]) \n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    ax[0].set_title('FOXA2')\n",
    "    sns.scatterplot(x=embedding_sox[idx,0], y=embedding_sox[idx,1], hue=labels[idx], palette=palette1, s=1.5, ax=ax[1])\n",
    "    sns.scatterplot(x=embedding_sox[names == sample,0], y=embedding_sox[names == sample,1], hue=names[names == sample], palette=['red'], s=1.5, ax=ax[1])\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "    ax[1].set_title('SOX17')\n",
    "    ax[1].legend([],[], frameon=False)\n",
    "    sns.scatterplot(x=embedding_oct[idx,0], y=embedding_oct[idx,1], hue=labels[idx], palette=palette1, s=1.5, ax=ax[2])\n",
    "    sns.scatterplot(x=embedding_oct[names == sample,0], y=embedding_oct[names == sample,1], hue=names[names == sample], palette=['red'], s=1.5, ax=ax[2])\n",
    "    ax[2].set_xticks([])\n",
    "    ax[2].set_yticks([])\n",
    "    ax[2].set_title('OCT4')\n",
    "    ax[2].legend([],[], frameon=False)\n",
    "    plt.suptitle(f'UMAP projection of {sample} against training batch')\n",
    "    ground_truth = annotations[annotations['sample'] == sample]['D4 pass'].values[0]\n",
    "    plt.savefig(f'umap_stdrd_train6/{sample}_{ground_truth}_train6.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folds = np.load('test_folds_super.npy', allow_pickle=True)\n",
    "train_folds = np.load('train_folds_super.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_folds[1]\n",
    "train_indices = np.isin(names, test_set, invert=True)\n",
    "print(test_set)\n",
    "\n",
    "train_fold = data[np.isin(samples, test_set, invert=True),:,:]\n",
    "test_fold = data[np.isin(samples, test_set),:,:]\n",
    "samples_train = samples[np.isin(samples, test_set, invert=True)]\n",
    "samples_test = samples[np.isin(samples, test_set)]\n",
    "\n",
    "train_fold_p = train_fold[annotations['D4 pass'].values[np.isin(samples, test_set, invert=True)] == 1]\n",
    "train_fold_n = train_fold[annotations['D4 pass'].values[np.isin(samples, test_set, invert=True)] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "exp = '587'\n",
    "palette1 = [sns.color_palette(\"Blues\")[-1], sns.color_palette(\"Blues\")[1]]\n",
    "palette2 = [sns.color_palette(\"tab10\")[3]]\n",
    "# sns.scatterplot(x=embedding[train_indices,0], y=embedding[train_indices,1], hue=labels[train_indices], palette=palette1, s=1.5)\n",
    "# sns.scatterplot(x=embedding[names == exp,0], y=embedding[names == exp,1], hue=names[names == exp], palette=['red'], s=1.5)   \n",
    "# plt.legend()\n",
    "# plt.title(f'UMAP projection of {exp} against training batch')\n",
    "# plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15,5)) \n",
    "sns.scatterplot(x=embedding_fox[train_indices,0], y=embedding_fox[train_indices,1], hue=labels[train_indices], palette=palette1, s=1.5, ax=ax[0])\n",
    "sns.scatterplot(x=embedding_fox[names == exp,0], y=embedding_fox[names == exp,1], hue=names[names == exp], palette=['red'], s=1.5, ax=ax[0]) \n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "ax[0].set_title('FOXA2')\n",
    "sns.scatterplot(x=embedding_sox[train_indices,0], y=embedding_sox[train_indices,1], hue=labels[train_indices], palette=palette1, s=1.5, ax=ax[1])\n",
    "sns.scatterplot(x=embedding_sox[names == exp,0], y=embedding_sox[names == exp,1], hue=names[names == exp], palette=['red'], s=1.5, ax=ax[1])\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_title('SOX17')\n",
    "ax[1].legend([],[], frameon=False)\n",
    "sns.scatterplot(x=embedding_oct[train_indices,0], y=embedding_oct[train_indices,1], hue=labels[train_indices], palette=palette1, s=1.5, ax=ax[2])\n",
    "sns.scatterplot(x=embedding_oct[names == exp,0], y=embedding_oct[names == exp,1], hue=names[names == exp], palette=['red'], s=1.5, ax=ax[2])\n",
    "ax[2].set_xticks([])\n",
    "ax[2].set_yticks([])\n",
    "ax[2].set_title('OCT4')\n",
    "ax[2].legend([],[], frameon=False)\n",
    "plt.suptitle(f'UMAP projection of {exp} against training batch')\n",
    "plt.savefig(f'umaps_{exp}_train6.png')\n",
    "plt.show()\n",
    "\n",
    "# # create 2x2 seaborn plot\n",
    "# fig, axs = plt.subplots(2, 2, figsize=(10,10))\n",
    "# for idx, channel in enumerate([0,1,2,14]):\n",
    "#     sns.kdeplot(train_fold_n.reshape(-1, train_fold.shape[2])[:,channel], color=palette1[0], ax=axs[idx//2,idx%2], label='0')\n",
    "#     # sns.histplot(train_fold_n.reshape(-1, train_fold.shape[2])[:,channel], kde=True, stat='density', bins=100, element=\"step\", fill=False, color=palette1[0], ax=axs[idx//2,idx%2], label='0')\n",
    "#     sns.kdeplot(train_fold_p.reshape(-1, train_fold.shape[2])[:,channel], color=palette1[1], ax=axs[idx//2,idx%2], label='1')\n",
    "#     # sns.histplot(train_fold_p.reshape(-1, train_fold.shape[2])[:,channel], kde=True, stat='density', bins=100, element=\"step\", fill=False, color=palette1[1], ax=axs[idx//2,idx%2], label='1')\n",
    "#     sns.kdeplot(test_fold[samples_test == exp].reshape(-1, test_fold.shape[2])[:,channel], color=palette2[0], ax=axs[idx//2,idx%2], label=exp)\n",
    "#     # sns.histplot(test_fold[samples_test == exp].reshape(-1, test_fold.shape[2])[:,channel], kde=True, stat='density', bins=100, element=\"step\", fill=False, color=palette2[0], ax=axs[idx//2,idx%2], label=exp)\n",
    "#     axs[idx//2,idx%2].legend()\n",
    "#     axs[idx//2,idx%2].set_title(f'{channels[channel]}')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
